{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMSMwBnsBcwoEenILNsskjA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7jGkQjzoSY_","executionInfo":{"status":"ok","timestamp":1706337518924,"user_tz":-330,"elapsed":26412,"user":{"displayName":"arun negi","userId":"11820845676501543929"}},"outputId":"398ab812-a9fc-4cd2-e5a3-f75c591cff42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Juj8Xhh1op-7","executionInfo":{"status":"ok","timestamp":1706337529498,"user_tz":-330,"elapsed":4,"user":{"displayName":"arun negi","userId":"11820845676501543929"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Get API keys"],"metadata":{"id":"Kw1mxRY34M9_"}},{"cell_type":"code","source":["from utils import get_circle_api_key\n","cci_api_key = get_circle_api_key()"],"metadata":{"id":"HIImxgGCpGa_","executionInfo":{"status":"ok","timestamp":1706341531349,"user_tz":-330,"elapsed":560,"user":{"displayName":"arun negi","userId":"11820845676501543929"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from utils import get_gh_api_key\n","gh_api_key = get_gh_api_key()"],"metadata":{"id":"SsKBEtE8xLEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils import get_openai_api_key\n","openai_api_key = get_openai_api_key()"],"metadata":{"id":"tWyA09j_4aFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setting up GitHub"],"metadata":{"id":"3R6Ii8EI4ihw"}},{"cell_type":"code","source":["from utils import get_repo_name\n","course_repo = get_repo_name()\n","course_repo"],"metadata":{"id":"Gzeg_j-A4c1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils import get_branch\n","course_branch = get_branch()\n","course_branch"],"metadata":{"id":"R7n-2SLw4oYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AI powered Quiz generator"],"metadata":{"id":"TxphKYd84rU9"}},{"cell_type":"code","source":["human_template  = \"{question}\""],"metadata":{"id":"BOikIyN-408-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n","   Categories: Art, Science\n","   Facts:\n","    - Painted the Mona Lisa\n","    - Studied zoology, anatomy, geology, optics\n","    - Designed a flying machine\n","\n","2. Subject: Paris\n","   Categories: Art, Geography\n","   Facts:\n","    - Location of the Louvre, the museum where the Mona Lisa is displayed\n","    - Capital of France\n","    - Most populous city in France\n","    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n","\n","3. Subject: Telescopes\n","   Category: Science\n","   Facts:\n","    - Device to observe different objects\n","    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n","    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n","\n","4. Subject: Starry Night\n","   Category: Art\n","   Facts:\n","    - Painted by Vincent van Gogh in 1889\n","    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n","\n","5. Subject: Physics\n","   Category: Science\n","   Facts:\n","    - The sun doesn't change color during sunset.\n","    - Water slows the speed of light\n","    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\""],"metadata":{"id":"FimGzJin44lZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the prompt template"],"metadata":{"id":"8V_tlnO65ETh"}},{"cell_type":"code","source":["delimiter = \"####\"\n","\n","prompt_template = f\"\"\"\n","Follow these steps to generate a customized quiz for the user.\n","The question will be delimited with four hashtags i.e {delimiter}\n","\n","The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n","should only refer to the category.\n","\n","Step 1:{delimiter} First identify the category user is asking about from the following list:\n","* Geography\n","* Science\n","* Art\n","\n","Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n","\n","{quiz_bank}\n","\n","Pick up to two subjects that fit the user's category.\n","\n","Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n","\n","Use the following format for the quiz:\n","Question 1:{delimiter} <question 1>\n","\n","Question 2:{delimiter} <question 2>\n","\n","Question 3:{delimiter} <question 3>\n","\n","\"\"\""],"metadata":{"id":"GdoOzkS35BMJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use langchin to build chat template"],"metadata":{"id":"DiPpMqvz5WrB"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])"],"metadata":{"id":"SLb1wzn95OLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print to observe the content or generated object\n","chat_prompt"],"metadata":{"id":"YYYlfpJ45bM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Choose LLM"],"metadata":{"id":"T3ke3sm15hS2"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","llm"],"metadata":{"id":"TYOS2WPL5eVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set up an output parser in LangChain that converts the llm response into a string."],"metadata":{"id":"-WIf14DJ5nI_"}},{"cell_type":"code","source":["# parser\n","from langchain.schema.output_parser import StrOutputParser\n","output_parser = StrOutputParser()\n","output_parser"],"metadata":{"id":"W8rRqlwr5jaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Connect the pieces using the pipe operator from Langchain Expression Language"],"metadata":{"id":"xiJVFXgq5w0n"}},{"cell_type":"code","source":["chain = chat_prompt | llm | output_parser\n","chain"],"metadata":{"id":"f6C7iHMC5u6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the function 'assistance_chain' to put together all steps above."],"metadata":{"id":"y06XJxW657wV"}},{"cell_type":"code","source":["# taking all components and making reusable as one piece\n","def assistant_chain(\n","    system_message,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  chat_prompt = ChatPromptTemplate.from_messages([\n","      (\"system\", system_message),\n","      (\"human\", human_template),\n","  ])\n","  return chat_prompt | llm | output_parser"],"metadata":{"id":"9QRevxBe50NC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluations\n","\n","Create the function 'eval_expected_words' for the first example."],"metadata":{"id":"9tLll8e_5_-Q"}},{"cell_type":"code","source":["def eval_expected_words(\n","    system_message,\n","    question,\n","    expected_words,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  assistant = assistant_chain(\n","      system_message,\n","      human_template,\n","      llm,\n","      output_parser)\n","\n","\n","  answer = assistant.invoke({\"question\": question})\n","\n","  print(answer)\n","\n","  assert any(word in answer.lower() \\\n","             for word in expected_words), \\\n","    f\"Expected the assistant questions to include \\\n","    '{expected_words}', but it did not\""],"metadata":{"id":"KeQRDEtc6GiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test: Generate a quiz about science."],"metadata":{"id":"gsHpLvxM6SNj"}},{"cell_type":"code","source":["question  = \"Generate a quiz about science.\"\n","expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"],"metadata":{"id":"cz_yfYGM6Om2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the eval."],"metadata":{"id":"17NO1SZw6aFI"}},{"cell_type":"code","source":["eval_expected_words(\n","    prompt_template,\n","    question,\n","    expected_words\n",")"],"metadata":{"id":"8R_REyAR6VZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the function 'evaluate_refusal' to define a failing test case where the app should decline to answer."],"metadata":{"id":"4tvMKdKz6gb7"}},{"cell_type":"code","source":["def evaluate_refusal(\n","    system_message,\n","    question,\n","    decline_response,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  assistant = assistant_chain(human_template,\n","                              system_message,\n","                              llm,\n","                              output_parser)\n","\n","  answer = assistant.invoke({\"question\": question})\n","  print(answer)\n","\n","  assert decline_response.lower() in answer.lower(), \\\n","    f\"Expected the bot to decline with \\\n","    '{decline_response}' got {answer}\""],"metadata":{"id":"1F-ZmZjP6b0p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a new question (which should be a bad request)"],"metadata":{"id":"Eel18KlC6noL"}},{"cell_type":"code","source":["question  = \"Generate a quiz about Rome.\"\n","decline_response = \"I'm sorry\""],"metadata":{"id":"dv3gMPvT6kjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the refusal eval."],"metadata":{"id":"FJZgWZKn6u_y"}},{"cell_type":"code","source":["evaluate_refusal(\n","    prompt_template,\n","    question,\n","    decline_response\n",")"],"metadata":{"id":"wGSfLWWs6uTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Running evaluations in a CircleCI pipeline**\n","\n","\n","Put all these steps together into files to reuse later.\n","\n","\n","Note: fixing the system_message by adding additional rules:\n","\n","\n","Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n","\n","If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\"."],"metadata":{"id":"EBMj83Or62DU"}},{"cell_type":"code","source":["%%writefile app.py\n","from langchain.prompts                import ChatPromptTemplate\n","from langchain.chat_models            import ChatOpenAI\n","from langchain.schema.output_parser   import StrOutputParser\n","\n","delimiter = \"####\"\n","\n","quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n","   Categories: Art, Science\n","   Facts:\n","    - Painted the Mona Lisa\n","    - Studied zoology, anatomy, geology, optics\n","    - Designed a flying machine\n","\n","2. Subject: Paris\n","   Categories: Art, Geography\n","   Facts:\n","    - Location of the Louvre, the museum where the Mona Lisa is displayed\n","    - Capital of France\n","    - Most populous city in France\n","    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n","\n","3. Subject: Telescopes\n","   Category: Science\n","   Facts:\n","    - Device to observe different objects\n","    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n","    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n","\n","4. Subject: Starry Night\n","   Category: Art\n","   Facts:\n","    - Painted by Vincent van Gogh in 1889\n","    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n","\n","5. Subject: Physics\n","   Category: Science\n","   Facts:\n","    - The sun doesn't change color during sunset.\n","    - Water slows the speed of light\n","    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n","\"\"\"\n","\n","system_message = f\"\"\"\n","Follow these steps to generate a customized quiz for the user.\n","The question will be delimited with four hashtags i.e {delimiter}\n","\n","The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n","should only refer to the category.\n","\n","Step 1:{delimiter} First identify the category user is asking about from the following list:\n","* Geography\n","* Science\n","* Art\n","\n","Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n","\n","{quiz_bank}\n","\n","Pick up to two subjects that fit the user's category.\n","\n","Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n","\n","Use the following format for the quiz:\n","Question 1:{delimiter} <question 1>\n","\n","Question 2:{delimiter} <question 2>\n","\n","Question 3:{delimiter} <question 3>\n","\n","Additional rules:\n","\n","- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n","- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n","\"\"\"\n","\n","\"\"\"\n","  Helper functions for writing the test cases\n","\"\"\"\n","\n","def assistant_chain(\n","    system_message=system_message,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  chat_prompt = ChatPromptTemplate.from_messages([\n","      (\"system\", system_message),\n","      (\"human\", human_template),\n","  ])\n","  return chat_prompt | llm | output_parser"],"metadata":{"id":"nP2-U6Pg7OoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Command to see the content:"],"metadata":{"id":"txUFTplV7ZBy"}},{"cell_type":"code","source":["!cat app.py"],"metadata":{"id":"x5Nbucwf7UXi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create new file to include the evals"],"metadata":{"id":"OWaf_GKS7b34"}},{"cell_type":"code","source":["%%writefile test_assistant.py\n","from app import assistant_chain\n","from app import system_message\n","from langchain.prompts                import ChatPromptTemplate\n","from langchain.chat_models            import ChatOpenAI\n","from langchain.schema.output_parser   import StrOutputParser\n","\n","import os\n","\n","from dotenv import load_dotenv, find_dotenv\n","_ = load_dotenv(find_dotenv())\n","\n","def eval_expected_words(\n","    system_message,\n","    question,\n","    expected_words,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  assistant = assistant_chain(system_message)\n","  answer = assistant.invoke({\"question\": question})\n","  print(answer)\n","\n","  assert any(word in answer.lower() \\\n","             for word in expected_words), \\\n","    f\"Expected the assistant questions to include \\\n","    '{expected_words}', but it did not\"\n","\n","def evaluate_refusal(\n","    system_message,\n","    question,\n","    decline_response,\n","    human_template=\"{question}\",\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    output_parser=StrOutputParser()):\n","\n","  assistant = assistant_chain(human_template,\n","                              system_message,\n","                              llm,\n","                              output_parser)\n","\n","  answer = assistant.invoke({\"question\": question})\n","  print(answer)\n","\n","  assert decline_response.lower() in answer.lower(), \\\n","    f\"Expected the bot to decline with \\\n","    '{decline_response}' got {answer}\"\n","\n","\"\"\"\n","  Test cases\n","\"\"\"\n","\n","def test_science_quiz():\n","\n","  question  = \"Generate a quiz about science.\"\n","  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n","  eval_expected_words(\n","      system_message,\n","      question,\n","      expected_subjects)\n","\n","def test_geography_quiz():\n","  question  = \"Generate a quiz about geography.\"\n","  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n","  eval_expected_words(\n","      system_message,\n","      question,\n","      expected_subjects)\n","\n","def test_refusal_rome():\n","  question  = \"Help me create a quiz about Rome\"\n","  decline_response = \"I'm sorry\"\n","  evaluate_refusal(\n","      system_message,\n","      question,\n","      decline_response)"],"metadata":{"id":"q3uFF5gv7bex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Command to see the content of the file:"],"metadata":{"id":"Ga64S9UE7lgw"}},{"cell_type":"code","source":["!cat test_assistant.py"],"metadata":{"id":"Mp85ZDRh7k3J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The CircleCI config file**\n","\n","\n","Now let's set up our tests to run automatically in CircleCI.\n","\n","For this course, we've created a working CircleCI config file. Let's take a look at the configuration."],"metadata":{"id":"wh3RtHg_7u09"}},{"cell_type":"code","source":["!cat circle_config.yml"],"metadata":{"id":"laW_72RM7rOc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Run the per-commit evals**\n","\n","Push files into the github repo."],"metadata":{"id":"tKZzpGyf74nr"}},{"cell_type":"code","source":["from utils import push_files\n","push_files(course_repo, course_branch, [\"app.py\", \"test_assistant.py\"])"],"metadata":{"id":"hkc4Qyma77sW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trigger the pipeline in CircleCI pipeline."],"metadata":{"id":"3ZQ8i4Tn8G-d"}},{"cell_type":"code","source":["from utils import trigger_commit_evals\n","trigger_commit_evals(course_repo, course_branch, cci_api_key)"],"metadata":{"id":"lFJa5JRV8Env"},"execution_count":null,"outputs":[]}]}